{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d1286-484b-4254-9df5-7ad0609e0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33315271-c78a-4511-882c-4b2138b8bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    return torch.stack([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2], dim=-1)\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    boxes1 = cxcywh_to_xyxy(boxes1)\n",
    "    boxes2 = cxcywh_to_xyxy(boxes2)\n",
    "\n",
    "    x1 = torch.max(boxes1[:, None, 0], boxes2[:, 0])\n",
    "    y1 = torch.max(boxes1[:, None, 1], boxes2[:, 1])\n",
    "    x2 = torch.min(boxes1[:, None, 2], boxes2[:, 2])\n",
    "    y2 = torch.min(boxes1[:, None, 3], boxes2[:, 3])\n",
    "\n",
    "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    return inter / union.clamp(min=1e-6)\n",
    "\n",
    "def hungarian_loss_batch(pred, target, class_weight=10.0, l1_weight=1.0, iou_weight=1.0):\n",
    "    \"\"\"\n",
    "    pred: (B, N, 5)\n",
    "    target: (B, M, 4)\n",
    "    \"\"\"\n",
    "    B, N, _ = pred.shape\n",
    "    M = target.shape[1]\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    for b in range(B):\n",
    "        pred_b = pred[b]      # (N, 5)\n",
    "        tgt_b = target[b]     # (M, 4)\n",
    "\n",
    "        distances = torch.norm(pred[b, :, 1:3], dim=1)\n",
    "        vesa = 1 + 1/(0.1 + (5*distances.detach())**2)\n",
    "\n",
    "        scores = torch.sigmoid(pred_b[:, 0])\n",
    "        boxes_pred = pred_b[:, 1:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cost_class = -scores[:, None]\n",
    "            cost_bbox = torch.cdist(boxes_pred, tgt_b, p=1)\n",
    "            cost_iou = -box_iou(boxes_pred, tgt_b)\n",
    "\n",
    "            total_cost = class_weight * cost_class + l1_weight * cost_bbox + iou_weight * cost_iou\n",
    "            indices = linear_sum_assignment(total_cost.cpu())\n",
    "\n",
    "        matched_idx_pred = torch.tensor(indices[0], device=pred.device)\n",
    "        matched_idx_tgt = torch.tensor(indices[1], device=pred.device)\n",
    "\n",
    "        # Score targets: matched → 1, unmatched → 0\n",
    "        target_scores = torch.zeros(N, device=pred.device)\n",
    "        target_scores[matched_idx_pred] = 1.\n",
    "\n",
    "        #print(scores, target_scores)\n",
    "        loss_score = F.binary_cross_entropy(scores, target_scores, weight=vesa)\n",
    "\n",
    "        # BBox & IoU losses (по matched)\n",
    "        matched_pred_boxes = boxes_pred[matched_idx_pred]\n",
    "        matched_target_boxes = tgt_b[matched_idx_tgt]\n",
    "\n",
    "        loss_bbox = F.mse_loss(matched_pred_boxes, matched_target_boxes)\n",
    "        loss_iou = 1 - box_iou(matched_pred_boxes, matched_target_boxes).mean()\n",
    "\n",
    "        loss = (\n",
    "            class_weight * loss_score +\n",
    "            l1_weight * loss_bbox +\n",
    "            iou_weight * loss_iou\n",
    "        )\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / B\n",
    "\n",
    "\n",
    "def batched_hungarian_loss(res1, bbox_target, views_masks, house_count, hungarian_loss_fn):\n",
    "    \"\"\"\n",
    "    res1: (B, V, N, 5) — предсказания боксов\n",
    "    bbox_target: (B, V, M, 4) — таргет боксы\n",
    "    views_masks: (B, V) — булева маска валидных views\n",
    "    house_count: (B,) — количество реальных боксов (M) на каждый B\n",
    "    hungarian_loss_fn: функция, принимающая (B', N, 5) и (B', M, 4)\n",
    "    \"\"\"\n",
    "    device = res1.device\n",
    "    B, V, N, _ = res1.shape\n",
    "    M = bbox_target.shape[2]\n",
    "\n",
    "    pred_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for bs in range(B):\n",
    "        valid_views = torch.where(views_masks[bs])[0]\n",
    "        for v in valid_views:\n",
    "            pred_boxes = res1[bs, v]  # (N, 5)\n",
    "            num_houses = house_count[bs]\n",
    "            targ_boxes = bbox_target[bs, v, :num_houses]  # (M, 4)\n",
    "\n",
    "            # Padding targets to match M if нужно\n",
    "            if num_houses < M:\n",
    "                pad = torch.zeros((M - num_houses, 4), device=device)\n",
    "                targ_boxes = torch.cat([targ_boxes, pad], dim=0)\n",
    "\n",
    "            pred_batch.append(pred_boxes)\n",
    "            target_batch.append(targ_boxes)\n",
    "\n",
    "    if len(pred_batch) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    pred_tensor = torch.stack(pred_batch)   # (B', N, 5)\n",
    "    target_tensor = torch.stack(target_batch)  # (B', M, 4)\n",
    "\n",
    "    return hungarian_loss_fn(pred_tensor, target_tensor)\n",
    "\n",
    "def batched_camera_loss(camera_target, res2, views_masks):\n",
    "    \"\"\"\n",
    "    camera_target: (B, V, V, D) — ground truth\n",
    "    res2: (B, V, V, D) — predicted\n",
    "    views_masks: (B, V) — bool mask per sample: which views are valid\n",
    "    \"\"\"\n",
    "    B = camera_target.shape[0]\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for bs in range(B):\n",
    "        mask = views_masks[bs]  # shape: (V,)\n",
    "        if mask.sum() == 0:\n",
    "            continue  # нет ни одного валидного представления\n",
    "\n",
    "        targ_points = camera_target[bs][mask][:, mask]  # shape: (m, m, D)\n",
    "        pred_points = res2[bs][mask][:, mask]\n",
    "\n",
    "        loss = F.mse_loss(pred_points, targ_points, reduction='sum')\n",
    "        total_loss += loss\n",
    "        total_count += targ_points.numel()  # нормализуем по числу элементов\n",
    "\n",
    "    if total_count == 0:\n",
    "        return torch.tensor(0.0, device=camera_target.device)\n",
    "\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48caaf5d-0614-474c-8707-965545e10860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pathes, v_len=64, h_len=16, f_dim=118):\n",
    "        self.pathes = pathes\n",
    "        self.f_dim = f_dim\n",
    "        self.v_len = v_len\n",
    "        self.h_len = h_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pathes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        with open(self.pathes[idx], 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rand_count_view = np.random.randint(len(data['v'])//2, len(data['v'])+1)\n",
    "        rand_views_list = sorted(random.sample(list(range(len(data['v']))), rand_count_view))\n",
    "        rand_house_fitr = torch.rand(data['n'], self.f_dim)\n",
    "        \n",
    "        padded = self.v_len-rand_count_view\n",
    "        n_houses = torch.tensor(data['t']).shape[1]\n",
    "        views_masks = torch.tensor([True]*rand_count_view+[False]*padded)\n",
    "        house_masks = []\n",
    "        house_fites = []\n",
    "        for view_idx in rand_views_list:\n",
    "            houses_f = []\n",
    "            h_c = len(data['b'][view_idx])\n",
    "            house_mask = torch.tensor([True]*h_c + [False]*(self.h_len-h_c))\n",
    "            \n",
    "            house_fitr = []\n",
    "            for house in torch.tensor(data['b'][view_idx]):\n",
    "                house_idx = int(house[0])\n",
    "                noise_house_f = rand_house_fitr[house_idx]# + torch.randn(self.f_dim) * 0.5\n",
    "                fully_house_f = torch.cat([house[1:], noise_house_f])\n",
    "                house_fitr.append(fully_house_f)\n",
    "            posfix = torch.full((self.h_len-len(data['b'][view_idx]), self.f_dim+4),-150.)\n",
    "            house_fitr = torch.cat([torch.stack(house_fitr), posfix], dim=0)\n",
    "            house_fites.append(house_fitr)\n",
    "            house_masks.append(house_mask)\n",
    "\n",
    "        postfix = torch.full((padded, self.h_len), False)\n",
    "        house_masks = torch.cat([torch.stack(house_masks), postfix], dim=0)\n",
    "        postfix = torch.full((padded, self.h_len, self.f_dim+4), -250.)\n",
    "        house_fites = torch.cat([torch.stack(house_fites), postfix], dim=0)\n",
    "\n",
    "        camera_real = torch.tensor(data['c'])[rand_views_list][:, rand_views_list]\n",
    "        target_real = torch.tensor(data['t'])[rand_views_list]\n",
    "\n",
    "        camera = torch.full((self.v_len, self.v_len, 2), -75.)\n",
    "        camera_idx = torch.cartesian_prod(torch.tensor(rand_views_list), torch.tensor(rand_views_list))\n",
    "        camera[camera_idx[:, 0], camera_idx[:, 1]] = camera_real.view(-1, 2)\n",
    "\n",
    "        target = torch.full((self.v_len, self.h_len, 4), -95.)\n",
    "        target[views_masks, :n_houses, :] = target_real[:,:,1:]\n",
    "\n",
    "        # target_masks -- views_masks, :n_houses, :\n",
    "        return house_fites, views_masks, house_masks, camera, target, n_houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b76b0-5bbf-483c-a8fc-deb18d34a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ... # Путь к сохраненным данным\n",
    "all_files = [data_path+'/'+i for i in os.listdir(data_path)]\n",
    "train_files, valid_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = Dataset(train_files)\n",
    "valid_data = Dataset(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9c436-71d0-4f9f-a343-f60a277273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_hidden_dim, transformer_dim, n_heads, ff_dim, num_layers, dropout=0.1, batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, transformer_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=batch_first)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.mlp(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016adbb-518a-4d3c-8e2d-9d2c803ad6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_hidden_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        def reshape_heads(tensor):\n",
    "            return tensor.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        Q = reshape_heads(Q)\n",
    "        K = reshape_heads(K)\n",
    "        V = reshape_heads(V)\n",
    "        raw_attention = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim**0.5\n",
    "        if src_key_padding_mask is not None:\n",
    "            # (B, 1, 1, S) — broadcast по головам и запросам\n",
    "            mask = src_key_padding_mask[:, None, None, :]  # True = pad\n",
    "            raw_attention = raw_attention.masked_fill(mask, 0.0)\n",
    "\n",
    "        return raw_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c06cfd-2786-4c21-87b4-d1df79b5e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_sigma1_sq = torch.nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma2_sq = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, loss1, loss2):\n",
    "        loss = (\n",
    "            0.5 * torch.exp(-self.log_sigma1_sq) * loss1 + 0.5 * self.log_sigma1_sq +\n",
    "            0.5 * torch.exp(-self.log_sigma2_sq) * loss2 + 0.5 * self.log_sigma2_sq\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_hidden_layers, output_dim, activation=nn.SiLU):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ce30b-4fe8-43a4-ad25-13317c1778a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.boxes = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5*16))\n",
    "        \n",
    "        self.house_encoder = Encoder(\n",
    "            input_dim=128,         # размерность входного вектора\n",
    "            mlp_hidden_dim=256,    # скрытый слой MLP\n",
    "            transformer_dim=128,   # d_model для трансформера\n",
    "            n_heads=4,             # количество голов внимания\n",
    "            ff_dim=128,            # размер FFN в трансформере\n",
    "            num_layers=2,          # количество слоёв трансформера\n",
    "        )\n",
    "\n",
    "        self.view_encoder = Encoder(\n",
    "            input_dim=128,         # размерность входного вектора\n",
    "            mlp_hidden_dim=256,    # скрытый слой MLP\n",
    "            transformer_dim=128,    # d_model для трансформера\n",
    "            n_heads=4,             # количество голов внимания\n",
    "            ff_dim=128,            # размер FFN в трансформере\n",
    "            num_layers=2,          # количество слоёв трансформера\n",
    "        )\n",
    "\n",
    "        self.camera = Matrix(\n",
    "            input_dim=128,\n",
    "            mlp_hidden_dim=256,\n",
    "            embed_dim=128,\n",
    "            num_heads=32\n",
    "        )\n",
    "\n",
    "        self.coords = nn.Sequential(\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2))\n",
    "\n",
    "    def forward(self, house_fites, views_masks, house_masks):\n",
    "        bs, nv, nh, _ = house_fites.shape\n",
    "        x, y = house_fites[..., 0], house_fites[..., 1]\n",
    "        rot0 = torch.stack([x, y], dim=-1)       # 0°\n",
    "        rot90 = torch.stack([-y, x], dim=-1)     # 90°\n",
    "        rot180 = torch.stack([-x, -y], dim=-1)   # 180°\n",
    "        rot270 = torch.stack([y, -x], dim=-1)    # 270°\n",
    "        all_rots = torch.cat([rot0, rot90, rot180, rot270, house_fites[..., 2:]], dim=-1)\n",
    "        house_seq = all_rots[views_masks]\n",
    "        all_rots[views_masks] = self.house_encoder(house_seq, src_key_padding_mask=~house_masks[views_masks])\n",
    "        view_vectors = all_rots.max(dim=2).values\n",
    "        final_view_vectors = self.view_encoder(view_vectors, src_key_padding_mask=~views_masks)\n",
    "        camera_matrix = self.camera(final_view_vectors, src_key_padding_mask=~views_masks)\n",
    "        camera_matrix = self.coords(camera_matrix.permute(0, 2, 3, 1))\n",
    "        boxes = self.boxes(final_view_vectors)\n",
    "        boxes = boxes.view(bs, nv, nh, 5)\n",
    "        \n",
    "        return boxes, camera_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c74ea-4086-4dd4-9ede-49aa3b4a7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_path = ... # Путь к весам модели\n",
    "model = Merge()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(w_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5fa48-2df8-4784-93db-e5377e8f16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01)\n",
    "\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741e3f9-6279-48d6-ae6e-cc199dc31bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = torch.inf\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoches: \"):\n",
    "    model.train()\n",
    "    total_train_camera, total_train_bboxes, total_train_loss = 0, 0, 0\n",
    "    train_bar = tqdm(train_loader, leave=False, desc=\"Train: \")\n",
    "\n",
    "    for batch in train_bar:\n",
    "        house_fites, views_masks, house_masks, camera_target, bbox_target, house_count = [b.to(device) for b in batch]\n",
    "        res1, res2 = model(house_fites, views_masks, house_masks)\n",
    "\n",
    "        #camera_loss = batched_camera_loss(camera_target, res2, views_masks)\n",
    "        bboxes_loss = batched_hungarian_loss(res1, bbox_target, views_masks, house_count, hungarian_loss_fn=hungarian_loss_batch)\n",
    "        #loss = multitask_loss_fn(camera_loss, bboxes_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        bboxes_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        #total_train_camera += camera_loss.item()\n",
    "        total_train_bboxes += bboxes_loss.item()\n",
    "        if bboxes_loss.item() < best_loss:\n",
    "            torch.save(model.state_dict(), 'w_mlp.pth')\n",
    "        #total_train_loss += loss.item()\n",
    "\n",
    "        train_bar.set_postfix({\n",
    "            #\"Loss\": f\"{loss.item():.4f}\",\n",
    "            #\"Cam\": f\"{camera_loss.item():.4f}\",\n",
    "            \"BBox\": f\"{bboxes_loss.item():.4f}\"\n",
    "        })\n",
    "    total_train_bboxes /= len(train_loader)\n",
    "    \n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_camera, val_bboxes, val_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, leave=False, desc=\"Valid: \"):\n",
    "            house_fites, views_masks, house_masks, camera_target, bbox_target, house_count = [b.to(device) for b in batch]\n",
    "            res1, res2 = model(house_fites, views_masks, house_masks)\n",
    "\n",
    "            #camera_loss = batched_camera_loss(camera_target, res2, views_masks)\n",
    "            bboxes_loss = batched_hungarian_loss(res1, bbox_target, views_masks, house_count, hungarian_loss_fn=hungarian_loss_batch)\n",
    "            #loss = multitask_loss_fn(camera_loss, bboxes_loss)\n",
    "\n",
    "            #val_camera += camera_loss.item()\n",
    "            val_bboxes += bboxes_loss.item()\n",
    "            #val_loss += loss.item()\n",
    "    val_bboxes /= len(valid_loader)\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    #print(f\"Train loss: total={total_train_loss:.4f}, cam={total_train_camera:.4f}, box={total_train_bboxes:.4f}\")\n",
    "    #print(f\"Val   loss: total={val_loss:.4f}, cam={val_camera:.4f}, box={val_bboxes:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train loss: box={total_train_bboxes:.4f}, Val   loss: box={val_bboxes:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
