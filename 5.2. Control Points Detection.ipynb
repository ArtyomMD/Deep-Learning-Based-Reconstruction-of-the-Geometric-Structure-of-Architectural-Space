{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8e33a-52e5-4a5f-ad28-1552bcbc4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea8d40-b0d0-457f-9383-67a171e07a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные компоненты обучения\n",
    "\n",
    "class FilesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, t_type, v_list, views, path_data, path_gen_img=None, path_gen_depth=None, deterministic=False):\n",
    "        self.t_type = t_type\n",
    "        self.v_list = v_list\n",
    "        self.views = views\n",
    "        self.path_data = path_data\n",
    "        self.path_gen_img = path_gen_img\n",
    "        self.path_gen_depth = path_gen_depth\n",
    "        if deterministic:\n",
    "            np.random.seed(1)\n",
    "        np.random.shuffle(self.v_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.v_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        view = self.v_list[idx]\n",
    "\n",
    "        p_json = self.path_data + '/' + view + '.json'\n",
    "        with open(p_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_info = json.load(f)\n",
    "\n",
    "        masks_dict = {}\n",
    "        for k, mask_name in self.views[view].items():\n",
    "            mask_path = self.path_data + '/' + mask_name\n",
    "            mask = torch.tensor(cv2.imread(mask_path)[:, :, 0] / 255).unsqueeze(0).to(torch.float32)\n",
    "            masks_dict[k] = mask\n",
    "\n",
    "        if self.t_type == 'sd':\n",
    "            p_depth = self.path_data + '/' + view + '_depth.jpg'\n",
    "            img_tensor = torch.tensor(cv2.imread(p_depth)[:, :, 0] / 255).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "        if self.t_type == 'img':\n",
    "            p_img = self.path_gen_img + '/' + view + '_gi.jpg'\n",
    "            img_tensor = torch.tensor(cv2.imread(p_img).permute(2, 0, 1) / 255).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "        if self.t_type == 'img':\n",
    "            p_img = self.path_gen_img + '/' + view + '_gi.jpg'\n",
    "            img = torch.tensor(cv2.imread(p_img).permute(2, 0, 1) / 255).unsqueeze(0).to(torch.float32)\n",
    "            p_depth = self.path_gen_depth + '/' + view + '_gd.jpg'\n",
    "            depth = torch.tensor(cv2.imread(p_depth)[:, :, 0] / 255).unsqueeze(0).to(torch.float32)\n",
    "            img_tensor = torch.cat([depth, img], dim=0)\n",
    "\n",
    "        return json_info, masks_dict, img_tensor\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    dict1_list = []\n",
    "    dict2_list = []\n",
    "    tensor_list = []\n",
    "\n",
    "    for dict1, dict2, tensor in batch:\n",
    "        dict1_list.append(dict1)\n",
    "        dict2_list.append(dict2)\n",
    "        tensor_list.append(tensor)\n",
    "    tensor_batch = torch.stack(tensor_list)\n",
    "\n",
    "    return dict1_list, dict2_list, tensor_batch\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device,\n",
    "               writer,\n",
    "               scheduler=None):\n",
    "    model.train()\n",
    "    loss_fn.train()\n",
    "    losses = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    for batch, (json_info, masks_dicts, img_tensor) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False,\n",
    "                                                            desc='Train'):\n",
    "        img_tensor = img_tensor.to(device)\n",
    "\n",
    "        back_mask, masks, mask_vectors, image_vector = model(img_tensor)\n",
    "        ancors_pred = extract_ancor_pred(mask_vectors)\n",
    "        ancors_json = [extract_ancor_dict(gt_json) for gt_json in json_info]\n",
    "        matching = hungarian_matching(ancors_pred, ancors_json)\n",
    "\n",
    "        l1 = point_loss(mask_vectors, json_info, matching)\n",
    "        l2 = aux_loss(mask_vectors, json_info, matching)\n",
    "        l3 = prob_loss(mask_vectors, json_info, matching)\n",
    "        l4 = masks_loss(back_mask, masks, masks_dicts, matching, json_info)\n",
    "        l5 = image_loss(image_vector, json_info)\n",
    "\n",
    "        l_list = [l1, l2, l3, l4, l5]\n",
    "        loss = loss_fn()\n",
    "        losses[0] += loss\n",
    "        for i in range(1, 6):\n",
    "            losses[i] = l_list[i].item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    for i in range(6):\n",
    "        losses[i] /= len(dataloader)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               device: torch.device,\n",
    "               writer,\n",
    "               scheduler = None):\n",
    "    model.train()\n",
    "    loss_fn.train()\n",
    "    losses = [0,0,0,0,0,0]\n",
    "\n",
    "    for batch, (json_info, masks_dicts, img_tensor) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False, desc='Train'):\n",
    "        img_tensor = img_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            back_mask, masks, mask_vectors, image_vector = model(img_tensor)\n",
    "        ancors_pred = extract_ancor_pred(mask_vectors)\n",
    "        ancors_json = [extract_ancor_dict(gt_json) for gt_json in json_info]\n",
    "        matching = hungarian_matching(ancors_pred, ancors_json)\n",
    "\n",
    "        l1 = point_loss(mask_vectors, json_info, matching)\n",
    "        l2 = aux_loss(mask_vectors, json_info, matching)\n",
    "        l3 = prob_loss(mask_vectors, json_info, matching)\n",
    "        l4 = masks_loss(back_mask, masks, masks_dicts, matching, json_info)\n",
    "        l5 = image_loss(image_vector, json_info)\n",
    "\n",
    "        l_list = [l1,l2,l3,l4,l5]\n",
    "        loss = loss_fn()\n",
    "        losses[0] += loss\n",
    "        for i in range(1, 6):\n",
    "            losses[i] = l_list[i].item()\n",
    "\n",
    "    for i in range(6):\n",
    "        losses[i] /= len(dataloader)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          path: str,\n",
    "          writer,\n",
    "          scheduler=None):\n",
    "    metrices = [\"train_loss_total\",\n",
    "                \"train_loss_point\",\n",
    "                \"train_loss_aux\",\n",
    "                \"train_loss_prob\",\n",
    "                \"train_loss_mask\",\n",
    "                \"train_loss_image\",\n",
    "                \"valid_loss_total\",\n",
    "                \"valid_loss_point\",\n",
    "                \"valid_loss_aux\",\n",
    "                \"valid_loss_prob\",\n",
    "                \"valid_loss_mask\",\n",
    "                \"valid_loss_image\"]\n",
    "\n",
    "    results = {k: [] for k in metrices}\n",
    "\n",
    "    model.to(device)\n",
    "    loss_fn.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_losses = train_step(model,\n",
    "                                  train_dataloader,\n",
    "                                  loss_fn,\n",
    "                                  optimizer,\n",
    "                                  device,\n",
    "                                  writer,\n",
    "                                  scheduler)\n",
    "        test_losses = test_step(model,\n",
    "                                test_dataloader,\n",
    "                                loss_fn,\n",
    "                                device,\n",
    "                                writer,\n",
    "                                scheduler)\n",
    "\n",
    "        ep = '000'+str(epoch + 1)[-3:]\n",
    "        stroka = [f\"Epoch: {ep}\"]\n",
    "        for i, l in enumerate(train_losses + test_losses):\n",
    "            results[metrices[i]].append(l)\n",
    "            s = f\"{metrices[i]}: {l:.3f}\"\n",
    "            stroka += [s]\n",
    "        print(' | '.join(stroka))\n",
    "\n",
    "        now = str(datetime.now())[:-7].replace(\" \", \"-\").replace(\":\", \"-\")\n",
    "        torch.save(model.state_dict(), path + f\"/{now}.pth\")\n",
    "        with open(path + f'/{now}.json', 'w') as json_file:\n",
    "            json.dump(results, json_file, indent=4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfad81-5c5e-4588-beea-1c462d1b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для расчёта ошибок при обучении\n",
    "\n",
    "def hungarian_matching(pred, targets):\n",
    "    \"\"\"\n",
    "    pred: (B, N, 3)\n",
    "    targets: list of B tensors of shape (M_i, 2)\n",
    "\n",
    "    Returns:\n",
    "        list of B tuples: (pred_indices, target_indices)\n",
    "    \"\"\"\n",
    "    batch_size = pred.shape[0]\n",
    "    assignments = []\n",
    "    for i in range(batch_size):\n",
    "        pred_points = pred[i, :, :2]  # (N, 2)\n",
    "        target_points = targets[i]  # (M, 2)\n",
    "\n",
    "        if target_points.shape[0] == 0:\n",
    "            assignments.append(([], []))\n",
    "            continue\n",
    "\n",
    "        # Вычисляем матрицу стоимостей (евклидово расстояние)\n",
    "        cost = torch.cdist(pred_points, target_points, p=2).cpu().numpy()  # shape: (N, M)\n",
    "        row_ind, col_ind = linear_sum_assignment(cost)\n",
    "        assignments.append((row_ind, col_ind))\n",
    "    # Сначала индекс предсказанного вектора, потом индекс целевого\n",
    "    return assignments\n",
    "\n",
    "# Получение якорей из одного json - на выходе формат переменный\n",
    "def extract_ancor_dict(gt_json):\n",
    "    ancors = []\n",
    "    for k in gt_json['houses'].keys():\n",
    "        s = list(gt_json['houses'][k].keys())[0]\n",
    "        line = torch.tensor(gt_json['houses'][k][s][1][:2]).to(torch.float)\n",
    "        ancor = line.mean(dim=0)\n",
    "        ancors += [ancor.tolist()]\n",
    "    return torch.tensor(ancors)\n",
    "\n",
    "# Получение якорей для всего батча - на выходе формат постоянный\n",
    "def extract_ancor_pred(vectors):\n",
    "    with torch.no_grad():\n",
    "        ancors = vectors.cpu()[:,:,-8:-4].view(*vectors.shape[:2], 2, 2).mean(dim=-2)\n",
    "    return ancors\n",
    "\n",
    "# Ошибка по нужным координатам для всего батча\n",
    "def point_loss(mask_vectors, gt_list, matching):\n",
    "    b, o, p = mask_vectors.shape\n",
    "    reals = torch.full((b,o,12), float('nan'))\n",
    "    if b != len(matching):\n",
    "        print('Не совпадают размеры')\n",
    "        return None\n",
    "    if b != len(matching):\n",
    "        print('Не совпадают размеры')\n",
    "        return None\n",
    "    for i in range(b):\n",
    "        gt_sam = gt_list[i]['houses']\n",
    "        hos = list(gt_sam.keys())\n",
    "        for p, match in list(zip(*matching[i])):\n",
    "            k = hos[match]\n",
    "            s = list(gt_sam[k].keys())[0]\n",
    "            reals[i, p, 4] = gt_sam[k][s][1][0][0]\n",
    "            reals[i, p, 5] = gt_sam[k][s][1][0][1]\n",
    "            reals[i, p, 6] = gt_sam[k][s][1][1][0]\n",
    "            reals[i, p, 7] = gt_sam[k][s][1][1][1]\n",
    "            if \"right\" in gt_sam[k].keys():\n",
    "                reals[i, p, 0] = gt_sam[k][\"right\"][1][2][0]\n",
    "                reals[i, p, 1] = gt_sam[k][\"right\"][1][2][1]\n",
    "                reals[i, p, 2] = gt_sam[k][\"right\"][1][3][0]\n",
    "                reals[i, p, 3] = gt_sam[k][\"right\"][1][3][1]\n",
    "            if \"left\" in gt_sam[k].keys():\n",
    "                reals[i, p, 8] = gt_sam[k][\"left\"][1][2][0]\n",
    "                reals[i, p, 9] = gt_sam[k][\"left\"][1][2][1]\n",
    "                reals[i, p, 10] = gt_sam[k][\"left\"][1][3][0]\n",
    "                reals[i, p, 11] = gt_sam[k][\"left\"][1][3][1]\n",
    "    reals = (reals-512)/512\n",
    "    mse = torch.nn.MSELoss()\n",
    "    mask = ~torch.isnan(reals)\n",
    "    preds = mask_vectors[:,:,5:]\n",
    "    loss = mse(preds[mask], reals.to(preds.device)[mask])\n",
    "    return loss\n",
    "\n",
    "# Ошибка по углу фасада для всего батча\n",
    "def aux_loss(mask_vectors, gt_list, matching):\n",
    "    b, o, p = mask_vectors.shape\n",
    "    reals = torch.full((b,o,2), float('nan'))\n",
    "    if b != len(matching):\n",
    "        print('Не совпадают размеры')\n",
    "        return None\n",
    "    for i in range(b):\n",
    "        gt_sam = gt_list[i]['houses']\n",
    "        hos = list(gt_sam.keys())\n",
    "        for p, match in list(zip(*matching[i])):\n",
    "            k = hos[match]\n",
    "            if \"right\" in gt_sam[k].keys():\n",
    "                reals[i, p, 0] = gt_sam[k][\"right\"][2]\n",
    "            if \"left\" in gt_sam[k].keys():\n",
    "                reals[i, p, 1] = gt_sam[k][\"left\"][2]\n",
    "    reals /= 90\n",
    "    mse = torch.nn.MSELoss()\n",
    "    mask = ~torch.isnan(reals)\n",
    "    preds = mask_vectors[:,:,3:5]\n",
    "    loss = mse(preds[mask], reals.to(preds.device)[mask])\n",
    "    return loss\n",
    "\n",
    "# Ошибка о вероятностям для всего батча\n",
    "def prob_loss(mask_vectors, gt_list, matching):\n",
    "    b, o, p = mask_vectors.shape\n",
    "    reals = torch.zeros(b, o, 3)\n",
    "    if b != len(matching):\n",
    "        print('Не совпадают размеры')\n",
    "        return None\n",
    "    for i in range(b):\n",
    "        reals[i, :, 0][matching[i][0]] = 1\n",
    "        gt_sam = gt_list[i]['houses']\n",
    "        hos = list(gt_sam.keys())\n",
    "        for p, match in list(zip(*matching[i])):\n",
    "            k = hos[match]\n",
    "            if \"right\" in gt_sam[k].keys():\n",
    "                reals[i, p, 1] = 1\n",
    "            if \"left\" in gt_sam[k].keys():\n",
    "                reals[i, p, 2] = 1\n",
    "    bce = torch.nn.BCEWithLogitsLoss()\n",
    "    preds = mask_vectors[:,:,:3]\n",
    "    loss = bce(preds, reals.to(preds.device))\n",
    "    return loss\n",
    "\n",
    "def angle_with_horizontal(o, v):\n",
    "    o = torch.tensor(o)\n",
    "    v = torch.tensor(v)\n",
    "    d = v - o  # Вектор отрезка\n",
    "    d_norm = torch.norm(d)\n",
    "    angle_rad = torch.arcsin(d[2] / d_norm)  # Z-компонента\n",
    "    angle_deg = angle_rad * 180 / torch.pi\n",
    "    return angle_deg.item()\n",
    "\n",
    "def image_loss(preds, gt):\n",
    "    mse = torch.nn.MSELoss()\n",
    "    angles = []\n",
    "    for i in gt:\n",
    "        angles.append([angle_with_horizontal(i['o'], i['v']), i['t']/90])\n",
    "    angles = torch.tensor(angles).to(torch.float32)\n",
    "    loss = mse(preds, angles.to(preds.device))\n",
    "    return loss\n",
    "\n",
    "# Ошибка масок по всему батчу: masks_dicts [{'0_left':_, '1_right':_}, ...]\n",
    "def masks_loss(back_mask, masks, masks_dicts, matching, gt_list):\n",
    "    loss = 0.0\n",
    "    count = 0\n",
    "    cel = torch.nn.CrossEntropyLoss()\n",
    "    b, o, c, h, w = masks.shape\n",
    "    if b != len(matching):\n",
    "        print('Не совпадают размеры')\n",
    "        return None\n",
    "    for i in range(b):\n",
    "        target_mask = []\n",
    "        predic_mask = []\n",
    "        gt_sam = gt_list[i]['houses']\n",
    "        hos = list(gt_sam.keys())\n",
    "        for p, match in list(zip(*matching[i])):\n",
    "            k = hos[match]\n",
    "            if \"right\" in gt_sam[k].keys():\n",
    "                target_mask.append(masks_dicts[i][k+'_right'])\n",
    "                predic_mask.append(masks[i, p, 0].unsqueeze(0))\n",
    "            if \"left\" in gt_sam[k].keys():\n",
    "                target_mask.append(masks_dicts[i][k+'_left'])\n",
    "                predic_mask.append(masks[i, p, 1].unsqueeze(0))\n",
    "        real_m_1 = torch.stack(target_mask, dim=0)\n",
    "        real_m_0 = (1-real_m_1.sum(dim=0)).unsqueeze(0)\n",
    "        real_m = torch.cat([real_m_0, real_m_1], dim=0)\n",
    "        pred_m_1 = torch.cat(predic_mask, dim=0)\n",
    "        pred_m_0 = back_mask[i]\n",
    "        pred_m = torch.cat([pred_m_0, pred_m_1], dim=0).unsqueeze(0)\n",
    "        real_idx = real_m.argmax(dim=0).long()\n",
    "        loss += cel(pred_m, real_idx.to(pred_m.device))\n",
    "        count += 1\n",
    "    return loss/count\n",
    "\n",
    "class LossWeighting(torch.nn.Module):\n",
    "    def __init__(self, num_losses):\n",
    "        super().__init__()\n",
    "        self.log_vars = torch.nn.Parameter(torch.zeros(num_losses))\n",
    "    def forward(self, losses):\n",
    "        total_loss = 0\n",
    "        for i, loss in enumerate(losses):\n",
    "            precision = torch.exp(-self.log_vars[i])\n",
    "            weighted_loss = precision * loss + self.log_vars[i]\n",
    "            total_loss += weighted_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f21ec4-5555-46e0-8dc0-690afa07ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модули модели\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MHAttentionMap(nn.Module):\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
    "        return weights\n",
    "\n",
    "def _expand(tensor, length: int):\n",
    "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
    "\n",
    "class MaskHeadConv(nn.Module):\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.dim = dim\n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 2, 3, padding=1)\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "    def forward(self, x, bbox_mask, fpns):\n",
    "        b, s, h, _, _ = bbox_mask.shape\n",
    "        x = torch.cat([_expand(x, s), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out_lay(x)\n",
    "\n",
    "        x = x.view(b, s*2, *x.shape[-2:])\n",
    "        x = F.interpolate(x, scale_factor=4, mode='nearest')\n",
    "        x = x.view(b, s, 2, *x.shape[-2:])\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, nheads=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nheads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_encoder_layers)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nheads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "\n",
    "    def forward(self, src, query_embed):\n",
    "        B, C, H, W = src.shape\n",
    "        src = src.flatten(2).permute(0, 2, 1)\n",
    "        memory = self.encoder(src)\n",
    "        tgt = query_embed.unsqueeze(0).expand(B, -1, -1)\n",
    "        hs = self.decoder(tgt, memory)\n",
    "        return hs, memory.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "\n",
    "class BLETR(nn.Module):\n",
    "    def __init__(self, num_q=10, in_channels=3, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "        self.n = nheads\n",
    "        self.q = num_q\n",
    "        return_nodes = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        if backbone.conv1.in_channels != in_channels:\n",
    "            backbone.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.feat = create_feature_extractor(backbone, return_nodes=return_nodes)\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        self.transformer = Transformer(hidden_dim, nheads,\n",
    "                                       num_encoder_layers, num_decoder_layers)\n",
    "        self.att_map = MHAttentionMap(hidden_dim, 256, nheads)\n",
    "        self.mask_head = MaskHeadConv(hidden_dim+nheads, [1024, 512, 256], hidden_dim)\n",
    "        self.query_pos = nn.Parameter(torch.rand(self.q, hidden_dim))\n",
    "        self.row_embed = nn.Parameter(torch.rand(hidden_dim // 2, 16))\n",
    "        self.col_embed = nn.Parameter(torch.rand(hidden_dim // 2, 16))\n",
    "        self.final = torch.nn.Conv2d(self.q*2, self.q*2, 3, padding=1)\n",
    "        self.background = nn.Conv2d(self.q*2, 1, 1)\n",
    "        self.bild_p = nn.Linear(hidden_dim, 1)\n",
    "        self.face_p = nn.Linear(hidden_dim, 2)\n",
    "        self.fo_aux = MLP(hidden_dim, [hidden_dim]*3, 2)\n",
    "        self.coords = MLP(hidden_dim, [hidden_dim]*3, 12)\n",
    "        self.dof = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim//2, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim//2, hidden_dim//4, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feat(x)\n",
    "        src = self.conv(features[\"layer4\"])\n",
    "        b, c, h, w = src.shape\n",
    "        pos = torch.cat([\n",
    "            self.row_embed.unsqueeze(1).repeat(1, h, 1),\n",
    "            self.col_embed.unsqueeze(2).repeat(1, 1, w)], dim=0).unsqueeze(0).repeat(b, 1,1,1)\n",
    "        hs, memory = self.transformer(src+pos, self.query_pos)\n",
    "        attn = self.att_map(hs, memory)\n",
    "        masks = self.mask_head(src, attn, [features[\"layer3\"], features[\"layer2\"], features[\"layer1\"]])\n",
    "        masks = self.final(masks.flatten(1,2))\n",
    "        back = self.background(masks)\n",
    "        masks = masks.view(b, self.q, 2, *masks.shape[-2:])\n",
    "        building = self.bild_p(hs)\n",
    "        faces = self.face_p(hs)\n",
    "        camera = self.fo_aux(hs)\n",
    "        points = self.coords(hs)\n",
    "        vectors = torch.cat([building, faces, camera, points], dim=-1)\n",
    "        image = torch.cat([src, memory], dim= 1)\n",
    "        image = self.dof(image)\n",
    "        return back, masks, vectors, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67174ef-60e9-4db3-a4bb-c5ada9ad5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(..., \"r\", encoding=\"utf-8\") as f:\n",
    "    v_list = json.load(f)\n",
    "\n",
    "with open(..., \"r\", encoding=\"utf-8\") as f:\n",
    "    views = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15452b-9ad2-4af3-bd12-6baf92efc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_views, valid_views = train_test_split(random.sample(v_list[:17476], 1000), test_size=0.3, random_state=42)\n",
    "#valid_views, test_views = train_test_split(lear_views, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = FilesDataset(t_type='sd', \n",
    "                    v_list=train_views, \n",
    "                    views=views, \n",
    "                    path_data=r'E:\\city_data')\n",
    "\n",
    "valid_dataset = FilesDataset(t_type='sd', \n",
    "                    v_list=valid_views, \n",
    "                    views=views, \n",
    "                    path_data=r'E:\\city_data')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=custom_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eccc43-4dda-434c-bec5-3439cfd0eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BLETR().cuda()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b019792-0cf7-4961-883d-16978e8f79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = LossWeighting(2)\n",
    "sum(p.numel() for p in loss_fn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385c262-131f-4fbf-9806-13031b36a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(list(model.parameters())+list(loss_fn.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=len(train_loader), epochs=n_epoches, pct_start=0.1)\n",
    "path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7a9ea-724e-4425-8913-5e86818d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(model, train_loader, valid_loader, optimizer, loss_fn, n_epoches, device, path, writer=None, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
