{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9941fce-42fc-45e9-a0fa-fec8e5016cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351d748-c176-4956-8149-455e099a8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_save(img_path, ann_path, tensor_path=False):\n",
    "    img_array = np.array(correct_exif_tags(Image.open(img_path)))\n",
    "    with open(ann_path, 'r') as file:\n",
    "        ann_json = json.load(file)\n",
    "    buildings = []\n",
    "    for house in ann_json['objects']:\n",
    "        cl = int(house['classTitle'][1:2])\n",
    "        if len(house['tags']) != 1 or cl == 4:\n",
    "            continue\n",
    "        floors = house['tags'][0]['value']\n",
    "        [x1, y1], [x2, y2] = house['points']['exterior']\n",
    "        build = img_array[y1:y2, x1:x2]\n",
    "        buildings.append((build, floors, cl))\n",
    "    if tensor_path:\n",
    "        for i, trio in enumerate(buildings):\n",
    "            name = tensor_path + f'__{i}h' + f'__{trio[2]}c' + f'__{trio[1]}f' + '.pth'\n",
    "            matrix, changes = standardize_image(trio[0])\n",
    "            matrix = torch.tensor(matrix, dtype=torch.uint8).permute(2,0,1)\n",
    "            torch.save((matrix, trio[1], changes), name)       \n",
    "    return buildings\n",
    "\n",
    "def record(source_path, final_path):\n",
    "    for uin in tqdm(os.listdir(source_path)):\n",
    "        if '.' in uin:\n",
    "            continue\n",
    "        uin_path = source_path + '/' + uin\n",
    "        ann_files = set([i[:-9] for i in os.listdir(uin_path + '/ann')])\n",
    "        img_files = set([i[:-4] for i in os.listdir(uin_path + '/img')])\n",
    "        files = list(ann_files & img_files)\n",
    "        for file in tqdm(files, leave=False):\n",
    "            img_path = uin_path + '/img/' + file + '.jpg'\n",
    "            ann_path = uin_path + '/ann/' + file + '.jpg.json'\n",
    "            tensor_path = final_path + ('/' if final_path != '' else '') + uin + '_' + file\n",
    "            crop_save(img_path, ann_path, tensor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02858c5-d50f-41b8-82bf-026bb8c5fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = ...\n",
    "final_path = ...\n",
    "record(source_path, final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5a877-dd2e-4e96-87ef-feec76f3e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilesDataset(Dataset):\n",
    "    def __init__(self, file_paths, deterministic=False):\n",
    "        self.file_paths = file_paths\n",
    "        if deterministic:\n",
    "            np.random.seed(1)\n",
    "        np.random.shuffle(self.file_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = torch.load(file_path, weights_only=True)\n",
    "        tensor_gray = (0.299 * data[0][0] + 0.587 * data[0][1] + 0.114 * data[0][2]).unsqueeze(0)\n",
    "        return tensor_gray, data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0aafa6-cf01-4add-89a4-8188b6c63a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [final_path + '/' + i for i in os.listdir(final_path)]\n",
    "uins = list(set([i.split('/')[-1].split('_')[0] for i in all_files]))\n",
    "train_uins, valid_uins = train_test_split(uins, test_size=0.2, random_state=42)\n",
    "train_files = [i for i in all_files if i.split('/')[-1].split('_')[0] in train_uins]\n",
    "valid_files = [i for i in all_files if i.split('/')[-1].split('_')[0] in valid_uins]\n",
    "\n",
    "train_df = FilesDataset(train_files)\n",
    "valid_df = FilesDataset(valid_files)\n",
    "\n",
    "train_dataloader = DataLoader(train_df, batch_size=10, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_df, batch_size=10, shuffle=False)\n",
    "\n",
    "model = models.resnet50()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(in_features=2048, out_features=1)\n",
    "\n",
    "num_epochs = 100\n",
    "device = 'cuda'\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=len(train_dataloader), epochs=num_epochs, pct_start=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839ac7c-f19a-4ff2-934d-a44766b4c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "writer = SummaryWriter(log_dir=\"logs\")\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, targets in tqdm(train_dataloader, leave=False):\n",
    "        images, targets = images.to(device)/255, targets.to(device)/1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    for images, targets in tqdm(valid_dataloader, leave=False):\n",
    "        images, targets = images.to(device)/255, targets.to(device)/1\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "    writer.add_scalars(\"Loss\", {\"train\": train_loss/len(train_dataloader), \"val\": valid_loss/len(valid_dataloader)}, epoch)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_dataloader):.4f}, Valid Loss: {valid_loss/len(valid_dataloader):.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebce1a-7213-4917-98b1-692dca88a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e759d9-466a-44d0-9708-a839fcd6dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "difs, y_true, y_pred = [0], [0], [0]\n",
    "for image, target in tqdm(valid_df):\n",
    "        image = image.unsqueeze(0).to(device)/255\n",
    "        with torch.no_grad():\n",
    "            out = round(model(image).item())\n",
    "            if out < 1:\n",
    "                out = 1\n",
    "            dif = out - target\n",
    "            difs.append(dif)\n",
    "        y_true.append(target)\n",
    "        y_pred.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d4aae-1a0c-4de3-9f8d-68632778604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(difs, bins = max(difs)-min(difs), edgecolor='black', color='grey')\n",
    "plt.xlabel('Разница предсказания и реальности')\n",
    "plt.ylabel('Частота')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fb4fb-8ab6-4b4d-a747-4c16b33ed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Настройки графика\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "\n",
    "# Добавляем цветовую шкалу\n",
    "plt.colorbar(cax, shrink=0.7)\n",
    "\n",
    "# Мелкие подписи осей\n",
    "ax.set_xticks(np.arange(len(cm)))\n",
    "ax.set_yticks(np.arange(len(cm)))\n",
    "ax.set_xticklabels(np.arange(len(cm)), fontsize=8)\n",
    "ax.set_yticklabels(np.arange(len(cm)), fontsize=8)\n",
    "\n",
    "# Названия осей\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"Actual\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
